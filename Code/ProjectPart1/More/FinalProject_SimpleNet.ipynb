{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject_SimpleNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DgYJ79CisQY",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=GtiQjkq8PnI6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ6l9dFnQkuW",
        "colab_type": "text"
      },
      "source": [
        "bootcamp:\n",
        "https://github.com/deeponcology/PyTorchMedicalAI\n",
        "https://github.com/deeponcology/Deep-Learning-Boot-Camp\n",
        "\n",
        "https://www.freecodecamp.org/news/how-i-used-deep-learning-to-classify-medical-images-with-fast-ai-cc4cfd64173c/\n",
        "\n",
        "\n",
        "https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "https://colab.research.google.com/drive/1N7r7HJ4ImgZNLXsSiuwCadVVwsGjLmFy\n",
        "\n",
        "https://colab.research.google.com/drive/1y0pgDW_0r4tPSk6URgWc3UekejIKBxDd#scrollTo=rmr9jTsazpR-\n",
        "\n",
        "https://medium.com/@ml_kid/custom-google-colab-notebooks-for-udacitys-deep-learning-with-pytorch-c4c6fbb04b6\n",
        "\n",
        "https://www.nature.com/articles/s41591-019-0447-x?fbclid=IwAR06RjSoEYhO6mfYcjt9NHXv7utULsDDfFOYFN-t4s5Xa4IWSwVjuiVNuME\n",
        "\n",
        "https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILHDxoVnZ1q4",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/deeponcology/PyTorchMedicalAI/blob/master/day1_CNN_basics.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxsmJE1PPCFj",
        "colab_type": "text"
      },
      "source": [
        "https://www.freecodecamp.org/news/how-to-build-the-best-image-classifier-3c72010b3d55/\n",
        "\n",
        "https://www.datascience.com/blog/transfer-learning-in-pytorch-part-one\n",
        "\n",
        "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=Ywspo1LiklkS\n",
        "\n",
        "https://colab.research.google.com/drive/1y0pgDW_0r4tPSk6URgWc3UekejIKBxDd#scrollTo=rmr9jTsazpR-\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=9bexQC8PaL9w\n",
        "\n",
        "https://colab.research.google.com/drive/1N7r7HJ4ImgZNLXsSiuwCadVVwsGjLmFy\n",
        "\n",
        "https://github.com/jamesdietle/fastaipart3/blob/master/Kvasir-Dataset2.ipynb\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa3dDqKoC2J5",
        "colab_type": "text"
      },
      "source": [
        "https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCObe1Pj4i9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Everything and ensure that during changes items are reloaded\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsPRhvxHn5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4d010a3d-94c5-4d0d-d99d-dff7e33c6000"
      },
      "source": [
        "#--------------1------------------\n",
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpE4uedgH7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a9ba8bb-d53e-4b9d-86e1-e70fb27ff13e"
      },
      "source": [
        "#--------------2------------------\n",
        "#Change Location\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive\")\n",
        "%ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " checkpoint.pth      \u001b[0m\u001b[01;34mDataSet1\u001b[0m/                'Getting started.pdf'\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/   \u001b[01;34mDataSet2\u001b[0m/                 \u001b[01;34mResearch_Shiba\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/              'DICOM Anonymizer .gdoc'  'Untitled presentation.gslides'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GyqUPhGynUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45fe5da8-addc-44b8-aa67-1b077a1d3a66"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDM_o8_z5dT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6ae7ddb6-d7a4-4a4e-821e-d53d8061ec81"
      },
      "source": [
        "#Git Clone\n",
        "#!git clone https://yoav-sagi-research:!Sy12345@github.com/yoav-sagi-research/DataSet1.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DataSet1'...\n",
            "remote: Enumerating objects: 755, done.\u001b[K\n",
            "remote: Counting objects: 100% (755/755), done.\u001b[K\n",
            "remote: Compressing objects: 100% (592/592), done.\u001b[K\n",
            "remote: Total 755 (delta 161), reused 755 (delta 161), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (755/755), 143.16 MiB | 13.66 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n",
            "Checking out files: 100% (742/742), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7UcKAgtNbx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "07b21987-ea4d-4245-be85-9c1ca7c7e5a4"
      },
      "source": [
        "\n",
        "#!pip3 install torch torchvision\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqCx_VtrOm2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c910e7b-08ac-4ab3-ebca-ca66a1f06bc5"
      },
      "source": [
        "#--------------3------------------\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    \n",
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgU5X31jOZgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#--------------4------------------\n",
        "#make IPython notebook matplotlib plot inline\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#Import packages\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as func\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "\n",
        "#from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if train_on_gpu else torch.LongTensor\n",
        "#Tensor = FloatTensor\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import time\n",
        "#from shutil import copyfile\n",
        "#from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "#from os import listdir, makedirs, getcwd, remove\n",
        "#from PIL import Image\n",
        "#from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import torch\n",
        "\n",
        "import random \n",
        "\n",
        "#import fastai\n",
        "#from fastai import *\n",
        "#import fastai more specific\n",
        "#from fastai.vision import *\n",
        "#from fastai.metrics import error_rate\n",
        "\n",
        "import copy #copy model\n",
        "import time #calc run time\n",
        "\n",
        "from collections import defaultdict #for class wize acc\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "#from torchvision.transforms import transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "import seaborn as sns #heatmap confsion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBc4Ic8LRJG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------5------------------\n",
        "import random \n",
        "# Set random seed for reproducability\n",
        "manualSeed = None\n",
        "\n",
        "manualSeed = 492345\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if train_on_gpu:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "if manualSeed is None:\n",
        "        manualSeed = 492345\n",
        "fixSeed(manualSeed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvEy8Kz1n2tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------6------------------\n",
        "#batch_size = 32\n",
        "#imageSize = 32\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20  #20 #$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "#batch_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWCn0VKVX0_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "45aa6b35-5548-4b00-aa1c-b11be801885b"
      },
      "source": [
        "print(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KosZMRYbUl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa774efd-d17e-4d7e-e12f-8316027f51e2"
      },
      "source": [
        "#----------------------- When testing with CIFAR10 -------------------- \n",
        "\n",
        "#Define transformations for the training set, flip the images randomly, crop out and apply mean and std normalization\n",
        "\n",
        "#First we pass an array of transformations using transform.Compose. \n",
        "#RandomHorizontalFlip randomly flips the images horizontally. \n",
        "#RandomCrop randomly crops the images. Below is an example of horizontal flipping.\n",
        "\n",
        "#ToTensor converts the images into a format usable by PyTorch. \n",
        "#Normalize with the values given below would make all our pixels range between -1 to +1. \n",
        "#Note that when stating the transformations, ToTensor and Normalize must be last in the exact order as defined above. \n",
        "#The primary reason for this is that the other transformations are applied on the input which is a PIL image, however, \n",
        "#this must be converted to a PyTorch tensor before applying normalization.\n",
        "\n",
        "# transform for the training data\n",
        "train_transformations = transforms.Compose([\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomCrop(imageSize,padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "#valid_transform = transforms.Compose([\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize([0.1307], [0.3081])\n",
        "#])\n",
        "\n",
        "#Load the training set\n",
        "train_set =CIFAR10(root=\"./data\",train=True,transform=train_transformations,download=True)\n",
        "\n",
        "# Load the valid set, \n",
        "# note that train is set to False\n",
        "valid_set = CIFAR10(root=\"./data\", train=False, transform=valid_transform, download=True)\n",
        "\n",
        "\n",
        "#print(train_set.train_data.shape)\n",
        "\n",
        "\n",
        "#Create a loder for the training set\n",
        "train_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "# Create a loder for the valid set, \n",
        "# shuffle is set to false for the valid loader\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXD9h9kACUZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------7------------------\n",
        "#Enables to load batch data with diffrent image sizes\n",
        "def customCollate(batch):\n",
        "    \n",
        "    compose = transforms.Compose(\n",
        "        [transforms.ToPILImage(),\n",
        "         transforms.Resize((image_size_width,image_size_hight))\n",
        "        ,transforms.ToTensor()\n",
        "        ])\n",
        "    #compose = transforms.Compose([transforms.Resize((image_size_width,image_size_hight))])\n",
        "    \n",
        "    imagesData = [compose(item[0]) for item in batch]    \n",
        "    imagesData = [torch.Tensor(item).to(device) for item in imagesData]\n",
        "    \n",
        "    #Can't use stack as it expect images from the same size\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    imagesData = torch.stack(imagesData, 0)\n",
        "    targetLabels = [item[1] for item in batch]\n",
        "    targetLabels = LongTensor(targetLabels)\n",
        "    return [imagesData, targetLabels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WAVOuLtWehiC",
        "colab": {}
      },
      "source": [
        "#--------------8------------------\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
        "        ''' Builds a feedforward network.        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "        #self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        #self.class_names = {str(k):v for k,v in enumerate(list(range(output_size)))} ##todo:\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "\n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            #x = self.bn(F.relu(each(x)))            \n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hde3eL__rtHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------9------------------\n",
        "#Dense model\n",
        "# Create the network, define the criterion and optimizer\n",
        "#784 input = image size of 28X28 , 10 ouput units layer , units in the hidden layer: 512, 256, 128\n",
        "#model = Network(784, 10, [512, 256, 128]) from udacity course\n",
        "\n",
        "# our ds: 445 X 510 or new files: 618 X 756 , or new 3D: 847X 1016\n",
        "#input: 226950 445 X 510 , 10 ouput units layer , units in the hidden layer: 512, 256, 128\n",
        "#            #$$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "#image_size_width = 445\n",
        "#image_size_hight = 510\n",
        "\n",
        "\n",
        "isMyDS = True #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$  \n",
        "\n",
        "if isMyDS:\n",
        "  size = 400  \n",
        "  image_size_width = size #44\n",
        "  image_size_hight = size #510\n",
        "  out_put_size = 5\n",
        "else:\n",
        "  size = 28             \n",
        "  image_size_width = 28\n",
        "  image_size_hight = 28\n",
        "  out_put_size = 10  \n",
        "  \n",
        "train_image_size = (size * size) #(image_size_width * image_size_hight)\n",
        "\n",
        "drop = 0.2\n",
        "#drop =  0   #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "model = Network(train_image_size, out_put_size, [512, 256, 128],drop)\n",
        "#https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
        "criterion = nn.NLLLoss() #Log Likelihood Loss same as  CrossEntropyLoss\n",
        "#learning_rate = 0.01         #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "#learning_rate = 0.001         #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "learning_rate= 0.005 * 2 * 2  #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "lr= 0.005 * 2 * 2\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,\n",
        "#                              weight_decay=0.0005, nesterov=True)\n",
        "\n",
        "model.to(device);\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbgol0QhP_Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------10------------------\n",
        "data_dir = 'DataSet1/data'\n",
        "# 445 X 510\n",
        "\n",
        "#if using imagenet as in tranfer learninig normalize diffrent then 0.5\n",
        "#https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/6\n",
        "# transforms.Normalize([0.485, 0.456, 0.406],\n",
        "#                      [0.229, 0.224, 0.225])]#\n",
        "\n",
        "# TODO: Define transfo\n",
        "\n",
        "\n",
        "#train_transforms = transforms.Compose([\n",
        "#                                       transforms.RandomRotation(30),\n",
        "#                                       transforms.RandomResizedCrop(224),\n",
        "#                                       transforms.RandomHorizontalFlip(),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                       #transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                       #                     [0.229, 0.224, 0.225])])\n",
        "\n",
        "#test_transforms = transforms.Compose([\n",
        "#                                      transforms.Resize(255),\n",
        "#                                      transforms.CenterCrop(224),\n",
        "#                                      transforms.ToTensor(),\n",
        "#                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                      #                     [0.229, 0.224, 0.225])])\n",
        "\n",
        "basic_tranforms = transforms.Compose([\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    #transforms.Resize(size),\n",
        "    transforms.CenterCrop(size), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=basic_tranforms) # train_transforms)\n",
        "valid_data = datasets.ImageFolder(data_dir + '/validation', transform=basic_tranforms) #  test_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=basic_tranforms)   #test_transforms)\n",
        "\n",
        "#trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "#testloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, \n",
        "                                           shuffle=True,\n",
        "                                           collate_fn=customCollate, # use custom collate function \n",
        "                                           #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                          )\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=customCollate, # use custom collate function \n",
        "                                          #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                          )\n",
        "                                      \n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=customCollate, # use custom collate function \n",
        "                                          #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                         )\n",
        "\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "#    sampler=train_sampler, num_workers=num_workers, shuffle=True)\n",
        "#valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "#    sampler=valid_sampler, num_workers=num_workers)\n",
        "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "#    num_workers=num_workers)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45JbeSiQtYQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------11------------------\n",
        "#calculate class-wise accuracies\n",
        "#from collections import defaultdict\n",
        "\n",
        "def update_classwise_accuracies(preds, labels, class_correct, class_totals):\n",
        "    correct_tensor = preds.eq(labels.data.view_as(preds))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy()) #np.squeeze: Remove single-dimensional entries from the shape of an array.\n",
        "    \n",
        "    #correct = np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(labels.shape[0]):\n",
        "        label = labels.data[i].item()\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_totals[label] += 1\n",
        "        \n",
        "        \n",
        "def get_accuracies(class_names,class_correct,class_totals):\n",
        "\n",
        "    accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
        "    class_accuracies = [(class_names[i],100.0*(class_correct[i]/class_totals[i]))\n",
        "                        for i in class_names.data if class_totals[i] > 0]\n",
        "    print('-' * 10)\n",
        "    \n",
        "    for i in range(len(class_names)):\n",
        "        label = class_names.data[i].item()\n",
        "        if class_totals[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                label, 100 * class_correct[i] / class_totals[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_totals[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (label))\n",
        "\n",
        "        print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * (np.sum(list(class_correct.values()))) / np.sum(list(class_totals.values())),\n",
        "        np.sum(list(class_correct.values())), np.sum(list(class_totals.values()))))\n",
        "    \n",
        "    print('-' * 10)\n",
        "    \n",
        "    return accuracy,class_accuracies\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd6mr0NtrB2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------12------------------\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "def printConfusionMatrix(labels, preds):  \n",
        "  labels = labels.tolist()  \n",
        "  preds =  preds.tolist()  \n",
        "  \n",
        "  plt.figure(figsize=(7,7))  \n",
        "  #y_true,y_pred = labels,[preds[i] for i in range(len(preds))]  #np.argmax: Returns the indices of the maximum values along an axis.\n",
        "  y_true,y_pred = labels,preds\n",
        "  fig = sns.heatmap(confusion_matrix(y_true,y_pred),cmap='RdBu',annot=True,linewidths=.25,linecolor='black')\n",
        "  fig.set_xlabel('actuals',fontsize=14)\n",
        "  fig.set_ylabel('predictions',fontsize=14)\n",
        "\n",
        "  print(classification_report(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBsBU4zsQja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------13------------------\n",
        "def train(device, inputSize, outputSize, model, trainloader, validloader, criterion, optimizer, epochs=5):\n",
        "    since = time.time()\n",
        "    \n",
        "    numberOfBatchForValidationForOverfitting = -1 #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    steps = 0\n",
        "    \n",
        "      \n",
        "    #running_loss = 0\n",
        "    #epoch_running_loss = 0\n",
        "    #valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "    train_losses_accumulate, valid_losses_accumulate = [], []\n",
        "    train_accuracy_accumulate, valid_accuracy_accumulate = [], []\n",
        "      \n",
        "    for epoch in range(epochs):\n",
        "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "            print('-' * 10)\n",
        "            train_running_loss,valid_running_loss, train_running_corrects,valid_running_corrects  = epochLoop(epoch, epochs, inputSize, model, trainloader,validloader, optimizer,criterion, numberOfBatchForValidationForOverfitting)\n",
        "            print('** Epoc Ended **')\n",
        "                      \n",
        "            \n",
        "            \n",
        "            train_losses_accumulate.append(train_running_loss)\n",
        "            valid_losses_accumulate.append(valid_running_loss) \n",
        "            train_accuracy_accumulate.append(train_running_corrects)\n",
        "            valid_accuracy_accumulate.append(valid_running_corrects)             \n",
        "            \n",
        "            totalTrainImages = len(train_loader.sampler)\n",
        "            if numberOfBatchForValidationForOverfitting != -1:  #$$$$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "              totalTrainImages = batch_size * numberOfBatchForValidationForOverfitting              \n",
        "            calcStatistics('Epoc','--- train ---', train_running_loss, train_running_corrects, totalTrainImages)\n",
        "            \n",
        "            totalValidImages = len(valid_loader.sampler)          \n",
        "            epoch_val_loss, epoch_val_acc = calcStatistics('Epoc',' -- validation -- ', valid_running_loss, valid_running_corrects, totalValidImages)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # deep copy the model\n",
        "            if epoch_val_acc > best_acc:\n",
        "                best_acc = epoch_val_acc\n",
        "                #$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                #save_Dense_net_checkpoint(inputSize, outputSize, model,'checkpoint.pth') # inputUnits, outPutUnits, model, fileName\n",
        "        \n",
        "           \n",
        "    time_elapsed = time.time() - since\n",
        "    \n",
        "    \n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "    \n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    \n",
        "    #get_accuracies(self.class_names,class_correct,class_totals)\n",
        "    \n",
        "    #plotMetrics(train_losses_accumulate, train_accuracy_accumulate, valid_losses_accumulate, valid_accuracy_accumulate)\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6tQMzALTyk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------14------------------\n",
        "\n",
        "def epochLoop(epoch, epochs, inputSize, model, trainloader,validloader, optimizer, criterion, numberOfBatchForValidationForOverfitting):\n",
        "        #epoch_running_loss = 0\n",
        "        #train_loss = 0.0\n",
        "        #train_acc = 0.0\n",
        "        steps=0\n",
        "        train_running_loss = 0.0\n",
        "        train_running_corrects = 0\n",
        "        \n",
        "        valid_running_loss = 0.0\n",
        "        valid_running_corrects = 0\n",
        "        printLoopInfo = True\n",
        "        printLoopDebug = False\n",
        "        \n",
        "        #class_correct = list(0. for i in range(outputSize))\n",
        "        #class_total = list(0. for i in range(outputSize))\n",
        "    \n",
        "        class_correct = defaultdict(int)\n",
        "        class_totals = defaultdict(int)\n",
        "        tran_preds =[]\n",
        "        \n",
        "        # Model in training mode, dropout is on\n",
        "        model.train()\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            if printLoopDebug:\n",
        "              print('BATCH STARTED, epoch:{} step:{} num of images:{}'.format(epoch, steps, len(images)))\n",
        "            if (numberOfBatchForValidationForOverfitting !=-1 and \n",
        "                steps == (numberOfBatchForValidationForOverfitting+1)):\n",
        "                print('BEARKING OUT,  steps:{}, num of batch crateria:{} '.format(steps, numberOfBatchForValidationForOverfitting))\n",
        "                steps=0\n",
        "                break\n",
        "                \n",
        "            #printBatchImage(images, labels)\n",
        "            \n",
        "                \n",
        "            #SprintBatch2(mean,std,images,labels)\n",
        "            \n",
        "             # Move input and label tensors to the default device\n",
        "             # As had diffrent images sizes , created a custom collect function, data already moved there to the device ,GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Flatten images into a width*hight image size long vector\n",
        "            #print('inputSize: ',inputSize)\n",
        "            images.resize_(images.size()[0], inputSize)\n",
        "            #print('@@ batch_size: {} step:{}, images len{} images shape: {}'.format(batch_size,steps,len(images),images.shape))\n",
        "           \n",
        "            #print('step:', steps)\n",
        "            # Flatten images\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "            \n",
        "            \n",
        "            # Clear all accumulated gradients\n",
        "            # This is important because weights in a neural network are adjusted based on gradients accumulated for each batch, \n",
        "            # hence for each new batch, gradients must be reset to zero, \n",
        "            # so images in a previous batch would not propagate gradients to a new batch.\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Retrieve the actual loss and then obtain the maximum predicted class.            \n",
        "            #train_loss += loss.cpu().data * images.size(0)  # images.size(0) = batch size\n",
        "            _, prediction = torch.max(output.data, 1)\n",
        "            \n",
        "            \n",
        "            ps = torch.exp(output)\n",
        "            # Class with highest probability is our predicted class, compare with true label            \n",
        "            equality = (labels.data == ps.max(1)[1])\n",
        "            # Accuracy is the number of correct predictions divided by all predictions, just take the mean           \n",
        "            accuracy = equality.type_as(torch.FloatTensor()).mean()\n",
        "            \n",
        "            \n",
        "            # statistics\n",
        "            lossMulByBatchSize = loss.item() * images.size(0) # images.size(0) = batch size \n",
        "            if printLoopDebug:\n",
        "              print('^^ Loss:{} lossMulByBatchSize:{} ^^ '.format(loss,lossMulByBatchSize))\n",
        "            train_running_loss += lossMulByBatchSize\n",
        "            corrects = torch.sum(prediction == labels.data)\n",
        "            train_running_corrects += corrects\n",
        "            if printLoopDebug:\n",
        "              print('Accuracy: ^^^ mean:{:.4f} corrects:{:.4f} ^^^'.format( accuracy,corrects/len(images) ))\n",
        "            \n",
        "                                         \n",
        "        model.eval()\n",
        "\n",
        "        # Turn off gradients for validation, will speed up inference\n",
        "        with torch.no_grad():\n",
        "            valid_loss, valid_corrects = validation(inputSize, model, validloader, criterion)\n",
        "\n",
        "        # statistics\n",
        "        valid_running_loss += valid_loss  \n",
        "        valid_running_corrects += valid_corrects           \n",
        "\n",
        "        # Make sure dropout and grads(learn the function for backprop) are on for training\n",
        "        model.train()\n",
        "        return train_running_loss,valid_running_loss, train_running_corrects,valid_running_corrects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXiSmR9NgCLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------15------------------\n",
        "def calcStatistics(state,phase, running_loss, running_corrects, dataset_sizes):\n",
        "    epoch_loss = np.float(running_loss) / dataset_sizes\n",
        "    epoch_acc = np.float(running_corrects) / dataset_sizes\n",
        "    \n",
        "    print('DS Size: {} state:{}, phase:{} ,running_loss:{}/{}, Loss: {:.4f} , running_corrects:{}/{} Acc: {:.4f}'.format(\n",
        "                dataset_sizes, state, phase, running_loss,dataset_sizes  ,epoch_loss, running_corrects,dataset_sizes, epoch_acc))\n",
        " \n",
        "    \n",
        "    return epoch_loss,epoch_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO4hAmdgsaRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------16------------------\n",
        "def validation(inputSize, model, validloader, criterion):\n",
        "    accuracy = 0\n",
        "    valid_loss = 0\n",
        "    for images, labels in validloader:\n",
        "        # Flatten images\n",
        "        #images = images.resize_(images.size()[0], 784)\n",
        "        #images = images.view(images.shape[0], -1)\n",
        "        images.resize_(images.size()[0], inputSize)\n",
        "        \n",
        "        # Move input and label tensors to the default device\n",
        "        # As had diffrent images sizes , created a custom collect function, data already moved there to the device ,GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        valid_loss += criterion(output, labels).item() * images.size(0)\n",
        "        #print('%%%%   VALID LOSS %%%%',valid_loss)\n",
        "        ## Calculating the accuracy \n",
        "        # Model's output is log-softmax, take exponential to get the probabilities\n",
        "        ps = torch.exp(output)\n",
        "        # Class with highest probability is our predicted class, compare with true label\n",
        "        equality = (labels.data == ps.max(1)[1])\n",
        "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "        _, prediction = torch.max(output.data, 1)\n",
        "        corrects = torch.sum(prediction == labels.data)\n",
        "           \n",
        "    return valid_loss, corrects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtesfJFdphn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------17------------------\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt result\n",
        "def plotMetrics(train_losses,train_acuuracy, valid_losses, valid_accuracy):\n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  plt.plot(valid_losses, label='Validation loss')\n",
        "  plt.plot(train_acuuracy, label='Training acuuracy')\n",
        "  plt.plot(valid_accuracy, label='Validation acuuracy')\n",
        "  plt.legend(frameon=False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY6T1nAMRBL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_Dense_net_checkpoint(inputUnits, outPutUnits, model, fileName):    \n",
        "    checkpoint = {'input_size': inputUnits,\n",
        "                  'output_size': outPutUnits,\n",
        "                  'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
        "                  'state_dict': model.state_dict()}\n",
        "\n",
        "    torch.save(checkpoint, fileName)\n",
        "    \n",
        "def load_Dense_net_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = Network(checkpoint['input_size'],\n",
        "                             checkpoint['output_size'],\n",
        "                             checkpoint['hidden_layers'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwMAUvJohakm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac8e709b-3ac6-4a8c-e418-16a778acd635"
      },
      "source": [
        "#--------------18------------------\n",
        "train(device, train_image_size, out_put_size ,model, train_loader, valid_loader, criterion, optimizer, epochs=200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:125318.91601800919/535, Loss: 234.2410 , running_corrects:146/535 Acc: 0.2729\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:139.04403030872345/91, Loss: 1.5280 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 1/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:1495.4115056991577/535, Loss: 2.7952 , running_corrects:213/535 Acc: 0.3981\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:125.32669770717621/91, Loss: 1.3772 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 2/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:781.8827199935913/535, Loss: 1.4615 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:119.50799322128296/91, Loss: 1.3133 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 3/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:1217.546747326851/535, Loss: 2.2758 , running_corrects:214/535 Acc: 0.4000\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:117.43147855997086/91, Loss: 1.2905 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 4/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:815.7334291934967/535, Loss: 1.5247 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:116.74593180418015/91, Loss: 1.2829 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 5/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:868.3588075637817/535, Loss: 1.6231 , running_corrects:214/535 Acc: 0.4000\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:115.8283103108406/91, Loss: 1.2728 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 6/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:754.2091149091721/535, Loss: 1.4097 , running_corrects:217/535 Acc: 0.4056\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:115.79861116409302/91, Loss: 1.2725 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 7/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:722.4611210823059/535, Loss: 1.3504 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:115.14747381210327/91, Loss: 1.2654 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 8/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:721.9202780723572/535, Loss: 1.3494 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.89403486251831/91, Loss: 1.2626 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 9/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:723.073553442955/535, Loss: 1.3515 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.71504557132721/91, Loss: 1.2606 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 10/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.5867153406143/535, Loss: 1.3469 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.8617115020752/91, Loss: 1.2622 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 11/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1151311397552/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.83396995067596/91, Loss: 1.2619 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 12/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.4319787025452/535, Loss: 1.3466 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.78981411457062/91, Loss: 1.2614 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 13/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0312441587448/535, Loss: 1.3459 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.72759771347046/91, Loss: 1.2607 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 14/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5477783679962/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.29442369937897/91, Loss: 1.2560 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 15/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3771731853485/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.46713787317276/91, Loss: 1.2579 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 16/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:761.1077976226807/535, Loss: 1.4226 , running_corrects:214/535 Acc: 0.4000\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2395960688591/91, Loss: 1.2554 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 17/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6136033535004/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.18711310625076/91, Loss: 1.2548 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 18/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7104775905609/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.45288467407227/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 19/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4950258731842/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.15521377325058/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 20/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4525736570358/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.45313256978989/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 21/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8468959331512/535, Loss: 1.3455 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.21405267715454/91, Loss: 1.2551 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 22/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:760.7699400186539/535, Loss: 1.4220 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.88876503705978/91, Loss: 1.2515 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 23/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:2180.1842683553696/535, Loss: 4.0751 , running_corrects:215/535 Acc: 0.4019\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16378700733185/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 24/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:774.5736277103424/535, Loss: 1.4478 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.29987895488739/91, Loss: 1.2560 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 25/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8255544900894/535, Loss: 1.3455 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.5194621682167/91, Loss: 1.2585 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 26/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:856.7192924022675/535, Loss: 1.6013 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.30236232280731/91, Loss: 1.2561 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 27/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:730.3672784566879/535, Loss: 1.3652 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16029489040375/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 28/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1237320899963/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2712253332138/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 29/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7816604375839/535, Loss: 1.3454 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.9147720336914/91, Loss: 1.2518 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 30/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1614043712616/535, Loss: 1.3442 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.69516772031784/91, Loss: 1.2604 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 31/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6849095821381/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.44525480270386/91, Loss: 1.2576 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 32/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8601847887039/535, Loss: 1.3455 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.74684143066406/91, Loss: 1.2500 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 33/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:723.7998282909393/535, Loss: 1.3529 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.27870750427246/91, Loss: 1.2558 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 34/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7689789533615/535, Loss: 1.3454 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.92262107133865/91, Loss: 1.2519 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 35/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3704241514206/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16716593503952/91, Loss: 1.2546 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 36/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.518746137619/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.47524112462997/91, Loss: 1.2580 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 37/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1744055747986/535, Loss: 1.3461 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.12982022762299/91, Loss: 1.2542 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 38/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1457837820053/535, Loss: 1.3442 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.22801166772842/91, Loss: 1.2553 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 39/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4555699825287/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.22514116764069/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 40/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3823611736298/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.22459602355957/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 41/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3186813592911/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.1266382932663/91, Loss: 1.2541 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 42/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:915.4407346248627/535, Loss: 1.7111 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.31914299726486/91, Loss: 1.2563 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 43/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.910694360733/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.7937930226326/91, Loss: 1.2505 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 44/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.4625827074051/535, Loss: 1.3467 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.37244290113449/91, Loss: 1.2568 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 45/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:895.6807851791382/535, Loss: 1.6742 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.79410403966904/91, Loss: 1.2505 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 46/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1532202959061/535, Loss: 1.3461 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.0046666264534/91, Loss: 1.2528 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 47/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.3607082366943/535, Loss: 1.3465 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.40084552764893/91, Loss: 1.2572 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 48/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3350219726562/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.17274588346481/91, Loss: 1.2546 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 49/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4417482614517/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.85999947786331/91, Loss: 1.2512 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 50/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5718109607697/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.45120400190353/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 51/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.2916008234024/535, Loss: 1.3463 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.89301931858063/91, Loss: 1.2516 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 52/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5823889970779/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26494091749191/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 53/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.446103811264/535, Loss: 1.3466 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.20805424451828/91, Loss: 1.2550 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 54/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.6041395664215/535, Loss: 1.3469 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26165145635605/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 55/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:718.5126090049744/535, Loss: 1.3430 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04971313476562/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 56/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.026792883873/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.41219919919968/91, Loss: 1.2573 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 57/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:779.0154147148132/535, Loss: 1.4561 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.92202776670456/91, Loss: 1.2519 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 58/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.719625711441/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.454873919487/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 59/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6497023105621/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.99512821435928/91, Loss: 1.2527 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 60/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5377188920975/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16568726301193/91, Loss: 1.2546 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 61/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6550339460373/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26048856973648/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 62/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2049014568329/535, Loss: 1.3443 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.98308038711548/91, Loss: 1.2526 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 63/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3524193763733/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.05121052265167/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 64/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5252722501755/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.06845712661743/91, Loss: 1.2535 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 65/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.2060264348984/535, Loss: 1.3462 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.97326374053955/91, Loss: 1.2525 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 66/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6134006977081/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.21729546785355/91, Loss: 1.2551 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 67/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:721.5174829959869/535, Loss: 1.3486 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.7944827079773/91, Loss: 1.2615 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 68/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.8754801750183/535, Loss: 1.3474 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.74435645341873/91, Loss: 1.2499 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 69/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2906039953232/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.3121994137764/91, Loss: 1.2562 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 70/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3848526477814/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.97774457931519/91, Loss: 1.2525 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 71/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2837202548981/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.0491156578064/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 72/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0434017181396/535, Loss: 1.3459 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.01169383525848/91, Loss: 1.2529 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 73/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5311510562897/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.95467382669449/91, Loss: 1.2522 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 74/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9147343635559/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16317129135132/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 75/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.5240243673325/535, Loss: 1.3468 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.19058990478516/91, Loss: 1.2548 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 76/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3950349092484/535, Loss: 1.3447 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.10566532611847/91, Loss: 1.2539 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 77/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.574465751648/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.01776307821274/91, Loss: 1.2529 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 78/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.8030009269714/535, Loss: 1.3473 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.16280281543732/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 79/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1069388389587/535, Loss: 1.3441 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.98346680402756/91, Loss: 1.2526 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 80/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.3357017040253/535, Loss: 1.3464 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.30452883243561/91, Loss: 1.2561 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 81/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4338643550873/535, Loss: 1.3447 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.87731063365936/91, Loss: 1.2514 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 82/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.664654135704/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.99996900558472/91, Loss: 1.2527 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 83/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3131047487259/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.62070739269257/91, Loss: 1.2596 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 84/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0857436656952/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.15824955701828/91, Loss: 1.2545 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 85/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4743132591248/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.1377182006836/91, Loss: 1.2543 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 86/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0915575027466/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.93313974142075/91, Loss: 1.2520 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 87/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2618352174759/535, Loss: 1.3444 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.02179580926895/91, Loss: 1.2530 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 88/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5419317483902/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.06875115633011/91, Loss: 1.2535 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 89/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1243404150009/535, Loss: 1.3442 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2263588309288/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 90/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1984903812408/535, Loss: 1.3443 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.17538547515869/91, Loss: 1.2547 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 91/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5206022262573/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.9412322640419/91, Loss: 1.2521 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 92/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.018560886383/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.29950141906738/91, Loss: 1.2560 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 93/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1573574542999/535, Loss: 1.3461 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.78282898664474/91, Loss: 1.2504 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 94/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3568241596222/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25210255384445/91, Loss: 1.2555 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 95/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3681234121323/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.01123923063278/91, Loss: 1.2529 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 96/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4957357645035/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.21910154819489/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 97/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3392312526703/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04173123836517/91, Loss: 1.2532 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 98/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:884.3959259986877/535, Loss: 1.6531 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.226666867733/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 99/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.2950203418732/535, Loss: 1.3463 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.97803962230682/91, Loss: 1.2525 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 100/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5266735553741/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.86821693181992/91, Loss: 1.2513 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 101/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8910462856293/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.14911663532257/91, Loss: 1.2544 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 102/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7055107355118/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.05636543035507/91, Loss: 1.2534 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 103/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3670582771301/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.12976551055908/91, Loss: 1.2542 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 104/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8743861913681/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.40332382917404/91, Loss: 1.2572 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 105/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8900204896927/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.93382358551025/91, Loss: 1.2520 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 106/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.0775156021118/535, Loss: 1.3441 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25105291604996/91, Loss: 1.2555 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 107/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5153135061264/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.18560665845871/91, Loss: 1.2548 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 108/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5233374834061/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.05018091201782/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 109/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.502004981041/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.01400429010391/91, Loss: 1.2529 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 110/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.6803697347641/535, Loss: 1.3471 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2691068649292/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 111/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1228487491608/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04299223423004/91, Loss: 1.2532 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 112/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.4837346076965/535, Loss: 1.3467 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.41381627321243/91, Loss: 1.2573 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 113/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0112229585648/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.00940722227097/91, Loss: 1.2529 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 114/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3067336082458/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.90803015232086/91, Loss: 1.2517 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 115/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6574193239212/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.47021824121475/91, Loss: 1.2579 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 116/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6777278184891/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25938254594803/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 117/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4907015562057/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.49273270368576/91, Loss: 1.2582 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 118/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.6548923254013/535, Loss: 1.3470 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.98010462522507/91, Loss: 1.2525 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 119/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.5925399065018/535, Loss: 1.3469 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.55872595310211/91, Loss: 1.2589 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 120/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7417563199997/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.70355921983719/91, Loss: 1.2495 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 121/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3844199180603/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.09913164377213/91, Loss: 1.2538 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 122/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.0550810098648/535, Loss: 1.3440 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.13686341047287/91, Loss: 1.2543 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 123/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9872261285782/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.23284322023392/91, Loss: 1.2553 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 124/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.0853416919708/535, Loss: 1.3441 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.58691090345383/91, Loss: 1.2592 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 125/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9158006906509/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.96544426679611/91, Loss: 1.2524 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 126/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8029524087906/535, Loss: 1.3454 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.33164221048355/91, Loss: 1.2564 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 127/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:996.2117302417755/535, Loss: 1.8621 , running_corrects:215/535 Acc: 0.4019\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04614716768265/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 128/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2776048183441/535, Loss: 1.3444 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.23798435926437/91, Loss: 1.2554 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 129/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.2177679538727/535, Loss: 1.3462 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.06165421009064/91, Loss: 1.2534 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 130/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5246183872223/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.17016988992691/91, Loss: 1.2546 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 131/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:742.2301143407822/535, Loss: 1.3873 , running_corrects:215/535 Acc: 0.4019\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.0420789718628/91, Loss: 1.2532 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 132/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.3369903564453/535, Loss: 1.3464 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.34639537334442/91, Loss: 1.2566 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 133/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:722.8660047054291/535, Loss: 1.3512 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.29029530286789/91, Loss: 1.2559 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 134/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5687061548233/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.13480985164642/91, Loss: 1.2542 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 135/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8820215463638/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.57702362537384/91, Loss: 1.2591 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 136/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6306526660919/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.17600047588348/91, Loss: 1.2547 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 137/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.2672890424728/535, Loss: 1.3444 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2026954293251/91, Loss: 1.2550 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 138/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5750069618225/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.67693740129471/91, Loss: 1.2492 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 139/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:718.8831716775894/535, Loss: 1.3437 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25204980373383/91, Loss: 1.2555 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 140/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7408121824265/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.45154863595963/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 141/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6719622612/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.242988884449/91, Loss: 1.2554 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 142/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:13236.988953948021/535, Loss: 24.7420 , running_corrects:217/535 Acc: 0.4056\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.86832803487778/91, Loss: 1.2513 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 143/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:995.9110444784164/535, Loss: 1.8615 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04179191589355/91, Loss: 1.2532 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 144/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4638949632645/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.06077527999878/91, Loss: 1.2534 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 145/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:1321.7434221506119/535, Loss: 2.4705 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.43999403715134/91, Loss: 1.2576 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 146/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:779.066259264946/535, Loss: 1.4562 , running_corrects:215/535 Acc: 0.4019\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.41349995136261/91, Loss: 1.2573 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 147/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8081678152084/535, Loss: 1.3454 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.73460948467255/91, Loss: 1.2498 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 148/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7226417064667/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26556015014648/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 149/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9903684854507/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.22248750925064/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 150/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4981575012207/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.51562774181366/91, Loss: 1.2584 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 151/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.086727142334/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.24934905767441/91, Loss: 1.2555 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 152/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:26966.140230298042/535, Loss: 50.4040 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.03261709213257/91, Loss: 1.2531 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 153/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6609288454056/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.1262537240982/91, Loss: 1.2541 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 154/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:5769.713709950447/535, Loss: 10.7845 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.20342510938644/91, Loss: 1.2550 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 155/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.573536157608/535, Loss: 1.3469 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.78656321763992/91, Loss: 1.2504 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 156/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5797151327133/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.58971112966537/91, Loss: 1.2592 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 157/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.4752287864685/535, Loss: 1.3448 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.06224071979523/91, Loss: 1.2534 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 158/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3561911582947/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.08348268270493/91, Loss: 1.2537 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 159/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7110289335251/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.09554147720337/91, Loss: 1.2538 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 160/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.559161067009/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.23553907871246/91, Loss: 1.2553 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 161/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3243277072906/535, Loss: 1.3445 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.86522740125656/91, Loss: 1.2513 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 162/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3738770484924/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.92069280147552/91, Loss: 1.2519 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 163/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1243207454681/535, Loss: 1.3442 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.4533925652504/91, Loss: 1.2577 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 164/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7525835037231/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.27703982591629/91, Loss: 1.2558 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 165/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5869201421738/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04836755990982/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 166/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6798431873322/535, Loss: 1.3452 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.0969249010086/91, Loss: 1.2538 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 167/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.6317851543427/535, Loss: 1.3451 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.22789651155472/91, Loss: 1.2553 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 168/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.0823513269424/535, Loss: 1.3441 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.10162878036499/91, Loss: 1.2539 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 169/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.368342757225/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.09522551298141/91, Loss: 1.2538 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 170/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7680044174194/535, Loss: 1.3454 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.86094808578491/91, Loss: 1.2512 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 171/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:836.5774828195572/535, Loss: 1.5637 , running_corrects:215/535 Acc: 0.4019\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25327718257904/91, Loss: 1.2555 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 172/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0556647777557/535, Loss: 1.3459 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.20220285654068/91, Loss: 1.2550 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 173/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5967036485672/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.04612499475479/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 174/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0126719474792/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.63641852140427/91, Loss: 1.2488 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 175/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5167148113251/535, Loss: 1.3449 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.57725894451141/91, Loss: 1.2591 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 176/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1813048124313/535, Loss: 1.3461 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.94678634405136/91, Loss: 1.2522 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 177/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8591369390488/535, Loss: 1.3455 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.13253778219223/91, Loss: 1.2542 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 178/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9094247817993/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26199513673782/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 179/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:9159.849934577942/535, Loss: 17.1212 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25766307115555/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 180/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3598800897598/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.25522947311401/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 181/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1928941011429/535, Loss: 1.3443 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.1263370513916/91, Loss: 1.2541 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 182/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.2387136220932/535, Loss: 1.3462 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.07827854156494/91, Loss: 1.2536 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 183/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.9794485569/535, Loss: 1.3476 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.05238515138626/91, Loss: 1.2533 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 184/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.9181046485901/535, Loss: 1.3475 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.36529695987701/91, Loss: 1.2568 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 185/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.49356341362/535, Loss: 1.3467 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.80012446641922/91, Loss: 1.2506 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 186/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.8221161365509/535, Loss: 1.3473 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.95042812824249/91, Loss: 1.2522 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 187/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.5965411663055/535, Loss: 1.3469 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.48719370365143/91, Loss: 1.2581 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 188/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0982302427292/535, Loss: 1.3460 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.78589457273483/91, Loss: 1.2504 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 189/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.296579003334/535, Loss: 1.3463 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.36018306016922/91, Loss: 1.2567 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 190/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:722.7036190032959/535, Loss: 1.3508 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.26644593477249/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 191/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.1604390144348/535, Loss: 1.3461 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.23677867650986/91, Loss: 1.2553 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 192/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.5886540412903/535, Loss: 1.3450 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.27285152673721/91, Loss: 1.2557 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 193/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.9989724159241/535, Loss: 1.3458 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.29478508234024/91, Loss: 1.2560 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 194/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7395128011703/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.05828189849854/91, Loss: 1.2534 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 195/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.7580081224442/535, Loss: 1.3453 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:113.90628379583359/91, Loss: 1.2517 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 196/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.1587507724762/535, Loss: 1.3442 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.36633485555649/91, Loss: 1.2568 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 197/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.8897022008896/535, Loss: 1.3456 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.21916592121124/91, Loss: 1.2552 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 198/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:719.3459248542786/535, Loss: 1.3446 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.4290509223938/91, Loss: 1.2575 , running_corrects:11/91 Acc: 0.1209\n",
            "Epoch 199/199\n",
            "----------\n",
            "** Epoc Ended **\n",
            "DS Size: 535 state:Epoc, phase:--- train --- ,running_loss:720.0541085004807/535, Loss: 1.3459 , running_corrects:216/535 Acc: 0.4037\n",
            "DS Size: 91 state:Epoc, phase: -- validation --  ,running_loss:114.2591905593872/91, Loss: 1.2556 , running_corrects:11/91 Acc: 0.1209\n",
            "Training complete in 30m 44s\n",
            "Best val Acc: 0.120879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (hidden_layers): ModuleList(\n",
              "    (0): Linear(in_features=160000, out_features=512, bias=True)\n",
              "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  )\n",
              "  (output): Linear(in_features=128, out_features=5, bias=True)\n",
              "  (dropout): Dropout(p=0.2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yy9bxkxebyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create a learning rate adjustment function that divides the learning rate by 10 every 30 epochs\n",
        "def adjust_learning_rate(epoch):\n",
        "    lr = 0.001\n",
        "\n",
        "    if epoch > 180:\n",
        "        lr = lr / 1000000\n",
        "    elif epoch > 150:\n",
        "        lr = lr / 100000\n",
        "    elif epoch > 120:\n",
        "        lr = lr / 10000\n",
        "    elif epoch > 90:\n",
        "        lr = lr / 1000\n",
        "    elif epoch > 60:\n",
        "        lr = lr / 100\n",
        "    elif epoch > 30:\n",
        "        lr = lr / 10\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg_wuCC1lniQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_basicCnn(model, trainloader, validloader, criterion, optimizer, numepochs=5):\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for epoch in range(numepochs):\n",
        "        # monitor training loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "        for data, target in train_loader:\n",
        "            # Move input and label tensors to the default device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update running training loss\n",
        "            train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        with torch.no_grad:\n",
        "              model.eval() # prep model for evaluation\n",
        "              for data, target in valid_loader:\n",
        "                  # Move input and label tensors to the default device\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "\n",
        "                  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "                  output = model(data)\n",
        "                  # calculate the loss\n",
        "                  loss = criterion(output, target)\n",
        "                  # update running validation loss \n",
        "                  valid_loss += loss.item()*data.size(0)\n",
        "      \n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        valid_loss = valid_loss/len(validloader.sampler)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch+1, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            torch.save(model.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uG1vtBdhSjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs):\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    # create a 1x1 grid to display the loss and progress\n",
        "    #grid = widgets.Grid(2,1)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        print('Epoch', epoch)\n",
        "        \n",
        "        # train phase\n",
        "        model.train()\n",
        "        \n",
        "        # create a progress bar\n",
        "        #progress = ProgressMonitor(length=len(train_set))\n",
        "        #train_losses_ma = []\n",
        "        #train_loss_ma = MovingAverage()\n",
        "        \n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "        # loop over the loader for the training set\n",
        "        for i, (batchImages, labels) in enumerate(train_loader):\n",
        "            # Move images and labels to gpu if available\n",
        "            # if GPU support is available,  move both the images and labels to the GPU\n",
        "            if cuda_avail:\n",
        "                images = Variable(images.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "            # Clear all accumulated gradients\n",
        "            # This is important because weights in a neural network are adjusted based on gradients accumulated for each batch, \n",
        "            # hence for each new batch, gradients must be reset to zero, \n",
        "            # so images in a previous batch would not propagate gradients to a new batch.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Pass our images into the model. \n",
        "            # It returns the predictions, \n",
        "            # and then we pass both the predictions and actual labels into the loss function.\n",
        "            \n",
        "            # forward propagation\n",
        "            # Predict classes\n",
        "            outputs_predictions = model(batchImages)\n",
        "            \n",
        "            # calculate the loss          \n",
        "            # Compute the loss based on the predictions and actual labels\n",
        "            loss = loss_fn(outputs_predictions, labels)\n",
        "            \n",
        "            # call loss.backward() to propagate the gradients, \n",
        "            # and call optimizer.step() to modify our model parameters in accordance with the propagated gradients\n",
        "           \n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "              \n",
        "            # update model weights\n",
        "            # Adjust parameters according to the computed gradients\n",
        "            optimizer.step()\n",
        "            \n",
        "            #compute the metrics:\n",
        "            \n",
        "            # update average loss\n",
        "            #train_loss_ma.update(loss)\n",
        "            \n",
        "            # Retrieve the actual loss and then obtain the maximum predicted class.            \n",
        "            train_loss += loss.cpu().data[0] * batchImages.size(0)\n",
        "            _, prediction = torch.max(outputs.data, 1)\n",
        "            # Sum up the number of correct predictions in the batch and add it to the total train_acc\n",
        "            train_acc += torch.sum(prediction == labels.data)\n",
        "            \n",
        "            # update progress bar\n",
        "            #with grid.output_to(0,0):\n",
        "            #    progress.update(batch.shape[0], train_loss)\n",
        "        \n",
        "        \n",
        "        #print('MA Training loss:', train_loss_ma)\n",
        "        #train_losses_ma.append(train_loss_ma.value)\n",
        "        \n",
        "        # Call the learning rate adjustment function\n",
        "        # After each epoch, call the learning rate adjustment function\n",
        "        adjust_learning_rate(epoch)\n",
        "        \n",
        "         \n",
        "        # Compute the training loss and training accuracy, over all # training images\n",
        "        train_acc = train_acc / trainImageCount\n",
        "        train_loss = train_loss / trainImageCount\n",
        "\n",
        "        # Evaluate on valid set\n",
        "        valid_acc = checkAccuracy(valid_loader, validImagesCount)\n",
        "        \n",
        "        # Save the model if the test acc is greater than our current best\n",
        "        if valid_acc > best_acc:\n",
        "            save_models(epoch)\n",
        "            best_acc = valid_acc\n",
        "        \n",
        "            # Save a checkpoint\n",
        "            checkpoint_filename = 'checkpoints/model-{:03d}.pkl'.format(epoch)\n",
        "            save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
        "        \n",
        "        \n",
        "        # Plot loss\n",
        "        #with grid.output_to(1, 0):\n",
        "        #    grid.clear_cell()\n",
        "        #    plt.figure(figsize=(10,6))\n",
        "        #    epochs = range(first_epoch, epoch + 1)\n",
        "        #    plt.plot(epochs, train_losses, '-o', label='Training loss')\n",
        "        #    plt.plot(epochs, valid_losses, '-o', label='Validation loss')\n",
        "        #    plt.legend()\n",
        "        #    plt.title('Learning curves')\n",
        "        #    plt.xlabel('Epoch')\n",
        "        #    plt.ylabel('Loss')\n",
        "        #    plt.xticks(epochs)\n",
        "        #    plt.show()\n",
        "        \n",
        "        # Evaluate on the test set\n",
        "        #test_acc = checkAccuracy(test_loader, trainImageCount)\n",
        "\n",
        "     \n",
        "        #if test_acc > best_acc:\n",
        "        #    save_models(epoch)\n",
        "        #    best_acc = test_acc\n",
        "\n",
        "        # Print the metrics\n",
        "        print(\"Epoch {}, Train Accuracy: {} , TrainLoss: {} , Test Accuracy: {}\".format(epoch, train_acc, train_loss,valid_acc))\n",
        "        \n",
        "        \n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DKpuTpqK0J",
        "colab_type": "text"
      },
      "source": [
        "#Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU3k4YaInu9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(num_epochs)\n",
        "#if __name__ == \"__main__\":\n",
        "#    train(num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "098kktNduQy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imagenet models are trained with image size 224\n",
        "# cifar10, are trained with image size 32\n",
        "#imageSize = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD83x_5ksZKh",
        "colab_type": "text"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phlfl5PVsboq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a8825d0c-e8cb-439c-cb65-434f368aff73"
      },
      "source": [
        "# Import needed packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import squeezenet1_1\n",
        "import torch.functional as F\n",
        "import requests\n",
        "import shutil\n",
        "from io import open\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "\"\"\" Instantiate model, this downloads tje 4.7 mb  squzzene the first time it is called.\n",
        "To use with your own model, re-define your trained networks ad load weights as below\n",
        "checkpoint = torch.load(\"pathtosavemodel\")\n",
        "model = SimpleNet(num_classes=10)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "$$$  Note that if your model was trained on ImageNet, then your num_classes must be 1000 instead of 10.\n",
        "$$$  Note if we’re running prediction with a model trained on cifar10, then in the transforms, change transforms.CenterCrop(224) to transforms.Resize(32)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = squeezenet1_1(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict_image(image_path):\n",
        "    print(\"Prediction in progress\")\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Define transformations for the image, should (note that imagenet models are trained with image size 224)\n",
        "    \n",
        "    transformation = transforms.Compose([\n",
        "        transforms.CenterCrop(imageSize),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Preprocess the image\n",
        "    image_tensor = transformation(image).float()\n",
        "\n",
        "    # Add an extra batch dimension since pytorch treats all images as batches\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image_tensor.cuda()\n",
        "\n",
        "    # Turn the input into a Variable\n",
        "    input = Variable(image_tensor)\n",
        "\n",
        "    # Predict the class of the image\n",
        "    output = model(input)\n",
        "\n",
        "    index = output.data.numpy().argmax()\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    imagefile = \"image.png\"\n",
        "\n",
        "    imagepath = os.path.join(os.getcwd(), imagefile)\n",
        "    # Donwload image if it doesn't exist\n",
        "    if not os.path.exists(imagepath):\n",
        "        data = requests.get(\n",
        "            \"https://github.com/OlafenwaMoses/ImageAI/raw/master/images/3.jpg\", stream=True)\n",
        "\n",
        "        with open(imagepath, \"wb\") as file:\n",
        "            shutil.copyfileobj(data.raw, file)\n",
        "\n",
        "        del data\n",
        "\n",
        "    index_file = \"class_index_map.json\"\n",
        "\n",
        "    indexpath = os.path.join(os.getcwd(), index_file)\n",
        "    # Donwload class index if it doesn't exist\n",
        "    if not os.path.exists(indexpath):\n",
        "        data = requests.get('https://github.com/OlafenwaMoses/ImageAI/raw/master/imagenet_class_index.json')\n",
        "\n",
        "        with open(indexpath, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(data.text)\n",
        "\n",
        "    class_map = json.load(open(indexpath))\n",
        "\n",
        "    # run prediction function annd obtain prediccted class index\n",
        "    index = predict_image(imagepath)\n",
        "\n",
        "    prediction = class_map[str(index)][1]\n",
        "\n",
        "    print(\"Predicted Class \", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\" to /root/.cache/torch/checkpoints/squeezenet1_1-f364aa15.pth\n",
            " 36%|███▌      | 1785856/4966400 [00:00<00:00, 11646119.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68ecea37c0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqueezenet1_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36msqueezenet1_1\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_squeezenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36m_squeezenet\u001b[0;34m(version, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'squeezenet'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0;32m--> 110\u001b[0;31m                                               progress=progress)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0m_download_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_download_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtXrCMHls9hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}