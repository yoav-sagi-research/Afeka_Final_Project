{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject_Clean.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DgYJ79CisQY",
        "colab_type": "text"
      },
      "source": [
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=GtiQjkq8PnI6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ6l9dFnQkuW",
        "colab_type": "text"
      },
      "source": [
        "bootcamp:\n",
        "https://github.com/deeponcology/PyTorchMedicalAI\n",
        "https://github.com/deeponcology/Deep-Learning-Boot-Camp\n",
        "\n",
        "https://www.freecodecamp.org/news/how-i-used-deep-learning-to-classify-medical-images-with-fast-ai-cc4cfd64173c/\n",
        "\n",
        "\n",
        "https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "https://colab.research.google.com/drive/1N7r7HJ4ImgZNLXsSiuwCadVVwsGjLmFy\n",
        "\n",
        "https://colab.research.google.com/drive/1y0pgDW_0r4tPSk6URgWc3UekejIKBxDd#scrollTo=rmr9jTsazpR-\n",
        "\n",
        "https://medium.com/@ml_kid/custom-google-colab-notebooks-for-udacitys-deep-learning-with-pytorch-c4c6fbb04b6\n",
        "\n",
        "https://www.nature.com/articles/s41591-019-0447-x?fbclid=IwAR06RjSoEYhO6mfYcjt9NHXv7utULsDDfFOYFN-t4s5Xa4IWSwVjuiVNuME\n",
        "\n",
        "https://www.pyimagesearch.com/2019/02/18/breast-cancer-classification-with-keras-and-deep-learning/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILHDxoVnZ1q4",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/deeponcology/PyTorchMedicalAI/blob/master/day1_CNN_basics.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxsmJE1PPCFj",
        "colab_type": "text"
      },
      "source": [
        "https://www.freecodecamp.org/news/how-to-build-the-best-image-classifier-3c72010b3d55/\n",
        "\n",
        "https://www.datascience.com/blog/transfer-learning-in-pytorch-part-one\n",
        "\n",
        "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=Ywspo1LiklkS\n",
        "\n",
        "https://colab.research.google.com/drive/1y0pgDW_0r4tPSk6URgWc3UekejIKBxDd#scrollTo=rmr9jTsazpR-\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo\n",
        "\n",
        "https://colab.research.google.com/drive/1gJAAN3UI9005ecVmxPun5ZLCGu4YBtLo#scrollTo=9bexQC8PaL9w\n",
        "\n",
        "https://colab.research.google.com/drive/1N7r7HJ4ImgZNLXsSiuwCadVVwsGjLmFy\n",
        "\n",
        "https://github.com/jamesdietle/fastaipart3/blob/master/Kvasir-Dataset2.ipynb\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "https://heartbeat.fritz.ai/basics-of-image-classification-with-pytorch-2f8973c51864"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa3dDqKoC2J5",
        "colab_type": "text"
      },
      "source": [
        "https://www.kdnuggets.com/2018/02/google-colab-free-gpu-tutorial-tensorflow-keras-pytorch.html/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCObe1Pj4i9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load Everything and ensure that during changes items are reloaded\n",
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsPRhvxHn5b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "98705b9e-3f5f-4b6c-9585-11f3c54f7853"
      },
      "source": [
        "#--------------1------------------\n",
        "#Mount Drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpE4uedgH7vx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bbf44636-9c15-4629-f4b8-28e8ed8afdb0"
      },
      "source": [
        "#--------------2------------------\n",
        "#Change Location\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive\")\n",
        "%ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " checkpoint.pth      \u001b[0m\u001b[01;34mDataSet1\u001b[0m/                'Getting started.pdf'\n",
            "\u001b[01;34m'Colab Notebooks'\u001b[0m/   \u001b[01;34mDataSet2\u001b[0m/                 \u001b[01;34mResearch_Shiba\u001b[0m/\n",
            " \u001b[01;34mdata\u001b[0m/              'DICOM Anonymizer .gdoc'  'Untitled presentation.gslides'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDM_o8_z5dT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6ae7ddb6-d7a4-4a4e-821e-d53d8061ec81"
      },
      "source": [
        "#Git Clone\n",
        "#!git clone https://yoav-sagi-research:!Sy12345@github.com/yoav-sagi-research/DataSet1.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DataSet1'...\n",
            "remote: Enumerating objects: 755, done.\u001b[K\n",
            "remote: Counting objects: 100% (755/755), done.\u001b[K\n",
            "remote: Compressing objects: 100% (592/592), done.\u001b[K\n",
            "remote: Total 755 (delta 161), reused 755 (delta 161), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (755/755), 143.16 MiB | 13.66 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n",
            "Checking out files: 100% (742/742), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7UcKAgtNbx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "07b21987-ea4d-4245-be85-9c1ca7c7e5a4"
      },
      "source": [
        "\n",
        "#!pip3 install torch torchvision\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqCx_VtrOm2y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20d098b8-0306-4cca-dcc1-c6b11512efdd"
      },
      "source": [
        "#--------------3------------------\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    \n",
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgU5X31jOZgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#--------------4------------------\n",
        "#make IPython notebook matplotlib plot inline\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "#Import packages\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as func\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "\n",
        "#from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
        "LongTensor = torch.cuda.LongTensor if train_on_gpu else torch.LongTensor\n",
        "#Tensor = FloatTensor\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "#import time\n",
        "#from shutil import copyfile\n",
        "#from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
        "#from os import listdir, makedirs, getcwd, remove\n",
        "#from PIL import Image\n",
        "#from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "#import torch\n",
        "\n",
        "import random \n",
        "\n",
        "#import fastai\n",
        "#from fastai import *\n",
        "#import fastai more specific\n",
        "#from fastai.vision import *\n",
        "#from fastai.metrics import error_rate\n",
        "\n",
        "import copy #copy model\n",
        "import time #calc run time\n",
        "\n",
        "from collections import defaultdict #for class wize acc\n",
        "\n",
        "from torchvision.datasets import CIFAR10\n",
        "#from torchvision.transforms import transforms\n",
        "#from torch.utils.data import DataLoader\n",
        "import seaborn as sns #heatmap confsion matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBc4Ic8LRJG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------5------------------\n",
        "import random \n",
        "# Set random seed for reproducability\n",
        "manualSeed = None\n",
        "\n",
        "manualSeed = 492345\n",
        "\n",
        "def fixSeed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if train_on_gpu:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "if manualSeed is None:\n",
        "        manualSeed = 492345\n",
        "fixSeed(manualSeed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvEy8Kz1n2tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------6------------------\n",
        "#batch_size = 32\n",
        "#imageSize = 32\n",
        "\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0\n",
        "# how many samples per batch to load\n",
        "batch_size = 20  #20 #$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "#batch_size = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWCn0VKVX0_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e7e4c33-8862-4864-ce8f-5dc0cba9cb15"
      },
      "source": [
        "print(batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KosZMRYbUl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fa774efd-d17e-4d7e-e12f-8316027f51e2"
      },
      "source": [
        "#----------------------- When testing with CIFAR10 -------------------- \n",
        "\n",
        "#Define transformations for the training set, flip the images randomly, crop out and apply mean and std normalization\n",
        "\n",
        "#First we pass an array of transformations using transform.Compose. \n",
        "#RandomHorizontalFlip randomly flips the images horizontally. \n",
        "#RandomCrop randomly crops the images. Below is an example of horizontal flipping.\n",
        "\n",
        "#ToTensor converts the images into a format usable by PyTorch. \n",
        "#Normalize with the values given below would make all our pixels range between -1 to +1. \n",
        "#Note that when stating the transformations, ToTensor and Normalize must be last in the exact order as defined above. \n",
        "#The primary reason for this is that the other transformations are applied on the input which is a PIL image, however, \n",
        "#this must be converted to a PyTorch tensor before applying normalization.\n",
        "\n",
        "# transform for the training data\n",
        "train_transformations = transforms.Compose([\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    #transforms.RandomCrop(imageSize,padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
        "])\n",
        "\n",
        "#valid_transform = transforms.Compose([\n",
        "#    transforms.ToTensor(),\n",
        "#    transforms.Normalize([0.1307], [0.3081])\n",
        "#])\n",
        "\n",
        "#Load the training set\n",
        "train_set =CIFAR10(root=\"./data\",train=True,transform=train_transformations,download=True)\n",
        "\n",
        "# Load the valid set, \n",
        "# note that train is set to False\n",
        "valid_set = CIFAR10(root=\"./data\", train=False, transform=valid_transform, download=True)\n",
        "\n",
        "\n",
        "#print(train_set.train_data.shape)\n",
        "\n",
        "\n",
        "#Create a loder for the training set\n",
        "train_loader = DataLoader(train_set,batch_size=batch_size,shuffle=True,num_workers=4)\n",
        "\n",
        "# Create a loder for the valid set, \n",
        "# shuffle is set to false for the valid loader\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXD9h9kACUZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------7------------------\n",
        "#Enables to load batch data with diffrent image sizes\n",
        "def customCollate(batch):\n",
        "    \n",
        "    compose = transforms.Compose(\n",
        "        [transforms.ToPILImage(),\n",
        "         transforms.Resize((image_size_width,image_size_hight))\n",
        "        ,transforms.ToTensor()\n",
        "        ])\n",
        "    #compose = transforms.Compose([transforms.Resize((image_size_width,image_size_hight))])\n",
        "    \n",
        "    imagesData = [compose(item[0]) for item in batch]    \n",
        "    imagesData = [torch.Tensor(item).to(device) for item in imagesData]\n",
        "    \n",
        "    #Can't use stack as it expect images from the same size\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    imagesData = torch.stack(imagesData, 0)\n",
        "    targetLabels = [item[1] for item in batch]\n",
        "    targetLabels = LongTensor(targetLabels)\n",
        "    return [imagesData, targetLabels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WAVOuLtWehiC",
        "colab": {}
      },
      "source": [
        "#--------------8------------------\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_layers, drop_p=0.5):\n",
        "        ''' Builds a feedforward network.        \n",
        "            Arguments\n",
        "            ---------\n",
        "            input_size: integer, size of the input layer\n",
        "            output_size: integer, size of the output layer\n",
        "            hidden_layers: list of integers, the sizes of the hidden layers\n",
        "        \n",
        "        '''\n",
        "        super().__init__()\n",
        "        # Input to a hidden layer\n",
        "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
        "        \n",
        "        # Add a variable number of more hidden layers\n",
        "        layer_sizes = zip(hidden_layers[:-1], hidden_layers[1:])\n",
        "        self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layer_sizes])\n",
        "        \n",
        "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=drop_p)\n",
        "        \n",
        "        #self.bn = nn.BatchNorm2d(num_features=out_channels)\n",
        "        #self.class_names = {str(k):v for k,v in enumerate(list(range(output_size)))} ##todo:\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ''' Forward pass through the network, returns the output logits '''\n",
        "\n",
        "        for each in self.hidden_layers:\n",
        "            x = F.relu(each(x))\n",
        "            #x = self.bn(F.relu(each(x)))            \n",
        "            x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hde3eL__rtHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------9------------------\n",
        "#Dense model\n",
        "# Create the network, define the criterion and optimizer\n",
        "#784 input = image size of 28X28 , 10 ouput units layer , units in the hidden layer: 512, 256, 128\n",
        "#model = Network(784, 10, [512, 256, 128]) from udacity course\n",
        "\n",
        "# our ds: 445 X 510 or new files: 618 X 756 , or new 3D: 847X 1016\n",
        "#input: 226950 445 X 510 , 10 ouput units layer , units in the hidden layer: 512, 256, 128\n",
        "#            #$$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "#image_size_width = 445\n",
        "#image_size_hight = 510\n",
        "\n",
        "\n",
        "isMyDS = True #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$  \n",
        "\n",
        "if isMyDS:\n",
        "  size = 445  \n",
        "  image_size_width = 445\n",
        "  image_size_hight = 510\n",
        "  out_put_size = 5\n",
        "else:\n",
        "  size = 28             \n",
        "  image_size_width = 28\n",
        "  image_size_hight = 28\n",
        "  out_put_size = 10  \n",
        "  \n",
        "train_image_size = (image_size_width * image_size_hight)\n",
        "\n",
        "drop = 0.2\n",
        "#drop =  0   #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "model = Network(train_image_size, out_put_size, [512, 256, 128],drop)\n",
        "#https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
        "criterion = nn.NLLLoss() #Log Likelihood Loss same as  CrossEntropyLoss\n",
        "#learning_rate = 0.01         #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "#learning_rate = 0.001         #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "learning_rate= 0.005 * 2 * 2  #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "lr= 0.005 * 2 * 2\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,\n",
        "#                              weight_decay=0.0005, nesterov=True)\n",
        "\n",
        "model.to(device);\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbgol0QhP_Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------10------------------\n",
        "data_dir = 'DataSet1/data'\n",
        "# 445 X 510\n",
        "\n",
        "#if using imagenet as in tranfer learninig normalize diffrent then 0.5\n",
        "#https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/6\n",
        "# transforms.Normalize([0.485, 0.456, 0.406],\n",
        "#                      [0.229, 0.224, 0.225])]#\n",
        "\n",
        "# TODO: Define transfo\n",
        "\n",
        "\n",
        "#train_transforms = transforms.Compose([\n",
        "#                                       transforms.RandomRotation(30),\n",
        "#                                       transforms.RandomResizedCrop(224),\n",
        "#                                       transforms.RandomHorizontalFlip(),\n",
        "#                                       transforms.ToTensor(),\n",
        "#                                       transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                       #transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                       #                     [0.229, 0.224, 0.225])])\n",
        "\n",
        "#test_transforms = transforms.Compose([\n",
        "#                                      transforms.Resize(255),\n",
        "#                                      transforms.CenterCrop(224),\n",
        "#                                      transforms.ToTensor(),\n",
        "#                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                      #transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                      #                     [0.229, 0.224, 0.225])])\n",
        "\n",
        "basic_tranforms = transforms.Compose([\n",
        "    #transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize(size),\n",
        "    #transforms.CenterCrop(size) \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=basic_tranforms) # train_transforms)\n",
        "valid_data = datasets.ImageFolder(data_dir + '/validation', transform=basic_tranforms) #  test_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=basic_tranforms)   #test_transforms)\n",
        "\n",
        "#trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "#testloader = torch.utils.data.DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, \n",
        "                                           shuffle=True,\n",
        "                                           collate_fn=customCollate, # use custom collate function \n",
        "                                           #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                          )\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=customCollate, # use custom collate function \n",
        "                                          #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                          )\n",
        "                                      \n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers,\n",
        "                                          collate_fn=customCollate, # use custom collate function \n",
        "                                          #pin_memory=True) #If you load your samples in the Dataset on CPU and would like to push it during training to the GPU, you can speed up the host to device transfer\n",
        "                                         )\n",
        "\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "#train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "#    sampler=train_sampler, num_workers=num_workers, shuffle=True)\n",
        "#valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "#    sampler=valid_sampler, num_workers=num_workers)\n",
        "#test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "#    num_workers=num_workers)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45JbeSiQtYQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------11------------------\n",
        "#calculate class-wise accuracies\n",
        "#from collections import defaultdict\n",
        "\n",
        "def update_classwise_accuracies(preds, labels, class_correct, class_totals):\n",
        "    correct_tensor = preds.eq(labels.data.view_as(preds))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy()) #np.squeeze: Remove single-dimensional entries from the shape of an array.\n",
        "    \n",
        "    #correct = np.squeeze(preds.eq(labels.data.view_as(preds)))\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(labels.shape[0]):\n",
        "        label = labels.data[i].item()\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_totals[label] += 1\n",
        "        \n",
        "        \n",
        "def get_accuracies(class_names,class_correct,class_totals):\n",
        "\n",
        "    accuracy = (100*np.sum(list(class_correct.values()))/np.sum(list(class_totals.values())))\n",
        "    class_accuracies = [(class_names[i],100.0*(class_correct[i]/class_totals[i]))\n",
        "                        for i in class_names.data if class_totals[i] > 0]\n",
        "    print('-' * 10)\n",
        "    \n",
        "    for i in range(len(class_names)):\n",
        "        label = class_names.data[i].item()\n",
        "        if class_totals[i] > 0:\n",
        "            print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "                label, 100 * class_correct[i] / class_totals[i],\n",
        "                np.sum(class_correct[i]), np.sum(class_totals[i])))\n",
        "        else:\n",
        "            print('Test Accuracy of %5s: N/A (no training examples)' % (label))\n",
        "\n",
        "        print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "        100. * (np.sum(list(class_correct.values()))) / np.sum(list(class_totals.values())),\n",
        "        np.sum(list(class_correct.values())), np.sum(list(class_totals.values()))))\n",
        "    \n",
        "    print('-' * 10)\n",
        "    \n",
        "    return accuracy,class_accuracies\n",
        "  \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd6mr0NtrB2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------12------------------\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "def printConfusionMatrix(labels, preds):  \n",
        "  labels = labels.tolist()  \n",
        "  preds =  preds.tolist()  \n",
        "  \n",
        "  plt.figure(figsize=(7,7))  \n",
        "  #y_true,y_pred = labels,[preds[i] for i in range(len(preds))]  #np.argmax: Returns the indices of the maximum values along an axis.\n",
        "  y_true,y_pred = labels,preds\n",
        "  fig = sns.heatmap(confusion_matrix(y_true,y_pred),cmap='RdBu',annot=True,linewidths=.25,linecolor='black')\n",
        "  fig.set_xlabel('actuals',fontsize=14)\n",
        "  fig.set_ylabel('predictions',fontsize=14)\n",
        "\n",
        "  print(classification_report(y_true,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBsBU4zsQja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------13------------------\n",
        "def train(device, inputSize, outputSize, model, trainloader, validloader, criterion, optimizer, epochs=5, print_every=10):\n",
        "    since = time.time()\n",
        "    \n",
        "    numberOfBatchForValidationForOverfitting = -1 #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    steps = 0\n",
        "    \n",
        "      \n",
        "    #running_loss = 0\n",
        "    #epoch_running_loss = 0\n",
        "    #valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "    train_losses_accumulate, valid_losses_accumulate = [], []\n",
        "    train_accuracy_accumulate, valid_accuracy_accumulate = [], []\n",
        "      \n",
        "    for epoch in range(epochs):\n",
        "            print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "            print('-' * 10)\n",
        "            train_running_loss,valid_running_loss, train_running_corrects,valid_running_corrects  = epochLoop(epoch, epochs, inputSize, model, trainloader,validloader, optimizer,criterion, numberOfBatchForValidationForOverfitting, print_every)\n",
        "            print('** Epoc Ended **')\n",
        "                      \n",
        "            \n",
        "            \n",
        "            train_losses_accumulate.append(train_running_loss)\n",
        "            valid_losses_accumulate.append(valid_running_loss) \n",
        "            train_accuracy_accumulate.append(train_running_corrects)\n",
        "            valid_accuracy_accumulate.append(valid_running_corrects)             \n",
        "            \n",
        "            totalTrainImages = len(train_loader.sampler)\n",
        "            if numberOfBatchForValidationForOverfitting != -1:\n",
        "              totalTrainImages = batch_size * numberOfBatchForValidationForOverfitting\n",
        "            calcStatistics('Epoc','--- train ---', train_running_loss, train_running_corrects, totalTrainImages)\n",
        "            \n",
        "            totalValidImages = len(valid_loader.sampler)\n",
        "            if numberOfBatchForValidationForOverfitting != -1:\n",
        "                totalValidImages = batch_size * numberOfBatchForValidationForOverfitting\n",
        "                \n",
        "            epoch_val_loss, epoch_val_acc = calcStatistics('Epoc',' -- validation -- ', valid_running_loss, valid_running_corrects, totalValidImages)\n",
        "            # deep copy the model\n",
        "            if epoch_val_acc > best_acc:\n",
        "                best_acc = epoch_val_acc\n",
        "                #$$$$$$$$$$$$$$$$$$$$$$$$$4\n",
        "                #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                #save_Dense_net_checkpoint(inputSize, outputSize, model,'checkpoint.pth') # inputUnits, outPutUnits, model, fileName\n",
        "        \n",
        "           \n",
        "            \n",
        "            #print(f\"Epoc Training loss: {epoch_running_loss/len(trainloader)}\") # the loader size is the total nubmber of data devided by batch size\n",
        "            ## print training/validation statistics \n",
        "            ## calculate average loss over an epoch\n",
        "            #print(f\"Total Training loss: {train_loss/len(trainloader.sampler)}\") #train sampler is the total number of data in train\n",
        "            ##print(f\"Valid loss: {valid_loss/len(validloader.sampler)}\") #valid sampler is the total number of data in valid\n",
        "            \n",
        "        # Compute the training loss and training accuracy, over all # training images\n",
        "        #print('train_acc:', train_acc/len(trainloader.sampler))    #train sampler is the total number of data in train        \n",
        "        #print('train_loss:', train_loss/len(trainloader.sampler))  #train sampler is the total number of data in train\n",
        "        \n",
        "    time_elapsed = time.time() - since\n",
        "    \n",
        "    \n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))\n",
        "    \n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    \n",
        "    #get_accuracies(self.class_names,class_correct,class_totals)\n",
        "    \n",
        "    #plotMetrics(train_losses_accumulate, train_accuracy_accumulate, valid_losses_accumulate, valid_accuracy_accumulate)\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6tQMzALTyk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------14------------------\n",
        "\n",
        "def epochLoop(epoch, epochs, inputSize, model, trainloader,validloader, optimizer, criterion, numberOfBatchForValidationForOverfitting, print_every):\n",
        "        #epoch_running_loss = 0\n",
        "        #train_loss = 0.0\n",
        "        #train_acc = 0.0\n",
        "        steps=0\n",
        "        train_running_loss = 0.0\n",
        "        train_running_corrects = 0\n",
        "        \n",
        "        valid_running_loss = 0.0\n",
        "        valid_running_corrects = 0\n",
        "        printLoopInfo = True\n",
        "        printLoopDebug = False\n",
        "        \n",
        "        #class_correct = list(0. for i in range(outputSize))\n",
        "        #class_total = list(0. for i in range(outputSize))\n",
        "    \n",
        "        class_correct = defaultdict(int)\n",
        "        class_totals = defaultdict(int)\n",
        "        tran_preds =[]\n",
        "        \n",
        "        # Model in training mode, dropout is on\n",
        "        model.train()\n",
        "        for images, labels in trainloader:\n",
        "            steps += 1\n",
        "            if printLoopDebug:\n",
        "              print('BATCH STARTED, epoch:{} step:{} num of images:{}'.format(epoch, steps, len(images)))\n",
        "            if (numberOfBatchForValidationForOverfitting !=-1 and \n",
        "                steps == (numberOfBatchForValidationForOverfitting+1)):\n",
        "                print('BEARKING OUT,  steps:{}, num of batch crateria:{} '.format(steps, numberOfBatchForValidationForOverfitting))\n",
        "                steps=0\n",
        "                break\n",
        "                \n",
        "            #printBatchImage(images, labels)\n",
        "            \n",
        "                \n",
        "            #SprintBatch2(mean,std,images,labels)\n",
        "            \n",
        "             # Move input and label tensors to the default device\n",
        "             # As had diffrent images sizes , created a custom collect function, data already moved there to the device ,GPU\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            # Flatten images into a width*hight image size long vector\n",
        "            #print('inputSize: ',inputSize)\n",
        "            images.resize_(images.size()[0], inputSize)\n",
        "            #print('@@ batch_size: {} step:{}, images len{} images shape: {}'.format(batch_size,steps,len(images),images.shape))\n",
        "           \n",
        "            #print('step:', steps)\n",
        "            # Flatten images\n",
        "            #images = images.view(images.shape[0], -1)\n",
        "            \n",
        "            \n",
        "            # Clear all accumulated gradients\n",
        "            # This is important because weights in a neural network are adjusted based on gradients accumulated for each batch, \n",
        "            # hence for each new batch, gradients must be reset to zero, \n",
        "            # so images in a previous batch would not propagate gradients to a new batch.\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model.forward(images)\n",
        "            loss = criterion(output, labels)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Retrieve the actual loss and then obtain the maximum predicted class.            \n",
        "            #train_loss += loss.cpu().data * images.size(0)  # images.size(0) = batch size\n",
        "            _, prediction = torch.max(output.data, 1)\n",
        "            \n",
        "            \n",
        "            ps = torch.exp(output)\n",
        "            # Class with highest probability is our predicted class, compare with true label            \n",
        "            equality = (labels.data == ps.max(1)[1])\n",
        "            # Accuracy is the number of correct predictions divided by all predictions, just take the mean           \n",
        "            accuracy = equality.type_as(torch.FloatTensor()).mean()\n",
        "            \n",
        "            #print('Probailites: ' ,ps.tolist())    #.cpu().detach().numpy())\n",
        "            #print('Probailites: ' ,[pred.item() for pred in ps.data])    #.cpu().detach().numpy())\n",
        "            #print('Labels: ', labels)\n",
        "            \n",
        "            \n",
        "            \n",
        "            # Sum up the number of correct predictions in the batch and add it to the total train_acc\n",
        "            #train_acc += torch.sum(prediction == labels.data)\n",
        "            #running_loss += loss.item() \n",
        "            #epoch_running_loss+= loss.item()\n",
        "            \n",
        "            # statistics\n",
        "            lossMulByBatchSize = loss.item() * images.size(0) # images.size(0) = batch size \n",
        "            if printLoopDebug:\n",
        "              print('^^ Loss:{} lossMulByBatchSize:{} ^^ '.format(loss,lossMulByBatchSize))\n",
        "            train_running_loss += lossMulByBatchSize\n",
        "            corrects = torch.sum(prediction == labels.data)\n",
        "            train_running_corrects += corrects\n",
        "            if printLoopDebug:\n",
        "              print('Accuracy: ^^^ mean:{:.4f} corrects:{:.4f} ^^^'.format( accuracy,corrects/len(images) ))\n",
        "            \n",
        "            \n",
        "            #update_classwise_accuracies(prediction, labels, class_correct, class_totals)\n",
        "            #get_accuracies(labels,class_correct,class_totals)\n",
        "            \n",
        "            #printConfusionMatrix(labels, prediction)\n",
        "            \n",
        "            #batchaccuracy = 100. * corrects / len(images)\n",
        "            \n",
        "            #print('batch images Size: {} ,len of images:{}, Loss: {:.4f} Loss*Size:{:.4f} Acc: {:.4f}'.format(\n",
        "            #     images.size(0), len(images), loss.item(),loss.item() * images.size(0), batchaccuracy)  )\n",
        "              \n",
        "            \n",
        "            #calcStatistics('Batch','train', train_running_loss, train_running_corrects, len(train_loader.sampler))\n",
        "            \n",
        "            if steps % print_every == 0:\n",
        "                if printLoopDebug:\n",
        "                  print('--enter validatin')\n",
        "                # Model in inference mode, dropout is off\n",
        "                model.eval()\n",
        "                \n",
        "                # Turn off gradients for validation, will speed up inference\n",
        "                with torch.no_grad():\n",
        "                    valid_loss, valid_acc = validation(inputSize, model, validloader, criterion)\n",
        "                \n",
        "                # statistics\n",
        "                valid_running_loss += valid_loss * images.size(0) # images.size(0) = batch size\n",
        "                valid_running_corrects += valid_acc\n",
        "                \n",
        "                \n",
        "                #calcStatistics('Batch','-- validation --', valid_running_loss, valid_running_corrects, len(valid_loader.sampler))\n",
        "                if printLoopInfo:\n",
        "                  print(\"INFO Debug, epochNum:{} Step:{} Print every: {} times via Loader Steps in single Epoch: {}/{}.. \".format(epoch, steps,print_every,epoch+1, epochs),\n",
        "                      \"Training Loss: {:.3f}.. \".format(train_running_loss/print_every),\n",
        "                      \"Valid Loss: {:.3f}.. \".format(valid_running_loss/len(validloader)),\n",
        "                      \"Valid Accuracy: {:.3f}\".format(valid_running_corrects/len(validloader)))\n",
        "                \n",
        "                \n",
        "                #running_loss = 0\n",
        "                \n",
        "                #running_valid_loss += valid_loss*data.size(0)\n",
        "                # Save the model if the test acc is greater than our current best\n",
        "                #if valid_acc > best_acc:\n",
        "                #    best_acc = valid_acc \n",
        "                #    \n",
        "                #    #save_models(epoch)                                               \n",
        "                #    save_Dense_net_checkpoint(inputSize, outputSize, model,'checkpoint.pth') # inputUnits, outPutUnits, model, fileName\n",
        "                #    # Save a checkpoint\n",
        "                #    #checkpoint_filename = 'checkpoints/model-{:03d}.pkl'.format(epoch)\n",
        "                #    #save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
        "                    \n",
        "\n",
        "                # Make sure dropout and grads(learn the function for backprop) are on for training\n",
        "                model.train()\n",
        "        return train_running_loss,valid_running_loss, train_running_corrects,valid_running_corrects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXiSmR9NgCLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------15------------------\n",
        "def calcStatistics(state,phase, running_loss, running_corrects, dataset_sizes):\n",
        "    epoch_loss = np.float(running_loss) / dataset_sizes\n",
        "    epoch_acc = np.float(running_corrects) / dataset_sizes\n",
        "    \n",
        "    print('DS Size: {} state:{}, phase:{} ,running_loss:{}/{}, Loss: {:.4f} , running_corrects:{}/{} Acc: {:.4f}'.format(\n",
        "                dataset_sizes, state, phase, running_loss,dataset_sizes  ,epoch_loss, running_corrects,dataset_sizes, epoch_acc))\n",
        " \n",
        "    \n",
        "    return epoch_loss,epoch_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YO4hAmdgsaRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------16------------------\n",
        "def validation(inputSize, model, validloader, criterion):\n",
        "    accuracy = 0\n",
        "    valid_loss = 0\n",
        "    for images, labels in validloader:\n",
        "        # Flatten images\n",
        "        #images = images.resize_(images.size()[0], 784)\n",
        "        #images = images.view(images.shape[0], -1)\n",
        "        images.resize_(images.size()[0], inputSize)\n",
        "        \n",
        "        # Move input and label tensors to the default device\n",
        "        # As had diffrent images sizes , created a custom collect function, data already moved there to the device ,GPU\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        output = model.forward(images)\n",
        "        valid_loss += criterion(output, labels).item()\n",
        "        #print('%%%%   VALID LOSS %%%%',valid_loss)\n",
        "        ## Calculating the accuracy \n",
        "        # Model's output is log-softmax, take exponential to get the probabilities\n",
        "        ps = torch.exp(output)\n",
        "        # Class with highest probability is our predicted class, compare with true label\n",
        "        equality = (labels.data == ps.max(1)[1])\n",
        "        # Accuracy is number of correct predictions divided by all predictions, just take the mean\n",
        "        accuracy += equality.type_as(torch.FloatTensor()).mean()\n",
        "\n",
        "    return valid_loss, accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtesfJFdphn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#--------------17------------------\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt result\n",
        "def plotMetrics(train_losses,train_acuuracy, valid_losses, valid_accuracy):\n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  plt.plot(valid_losses, label='Validation loss')\n",
        "  plt.plot(train_acuuracy, label='Training acuuracy')\n",
        "  plt.plot(valid_accuracy, label='Validation acuuracy')\n",
        "  plt.legend(frameon=False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY6T1nAMRBL-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_Dense_net_checkpoint(inputUnits, outPutUnits, model, fileName):    \n",
        "    checkpoint = {'input_size': inputUnits,\n",
        "                  'output_size': outPutUnits,\n",
        "                  'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
        "                  'state_dict': model.state_dict()}\n",
        "\n",
        "    torch.save(checkpoint, fileName)\n",
        "    \n",
        "def load_Dense_net_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = Network(checkpoint['input_size'],\n",
        "                             checkpoint['output_size'],\n",
        "                             checkpoint['hidden_layers'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwMAUvJohakm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cfa80043-0133-4cd7-a31a-ac20d80ec7a4"
      },
      "source": [
        "#--------------18------------------\n",
        "train(device, train_image_size, out_put_size ,model, train_loader, valid_loader, criterion, optimizer, epochs=200, print_every=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/199\n",
            "----------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yy9bxkxebyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create a learning rate adjustment function that divides the learning rate by 10 every 30 epochs\n",
        "def adjust_learning_rate(epoch):\n",
        "    lr = 0.001\n",
        "\n",
        "    if epoch > 180:\n",
        "        lr = lr / 1000000\n",
        "    elif epoch > 150:\n",
        "        lr = lr / 100000\n",
        "    elif epoch > 120:\n",
        "        lr = lr / 10000\n",
        "    elif epoch > 90:\n",
        "        lr = lr / 1000\n",
        "    elif epoch > 60:\n",
        "        lr = lr / 100\n",
        "    elif epoch > 30:\n",
        "        lr = lr / 10\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg_wuCC1lniQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_basicCnn(model, trainloader, validloader, criterion, optimizer, numepochs=5):\n",
        "    # initialize tracker for minimum validation loss\n",
        "    valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
        "\n",
        "    for epoch in range(numepochs):\n",
        "        # monitor training loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train() # prep model for training\n",
        "        for data, target in train_loader:\n",
        "            # Move input and label tensors to the default device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data)\n",
        "            # calculate the loss\n",
        "            loss = criterion(output, target)\n",
        "            # backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # update running training loss\n",
        "            train_loss += loss.item()*data.size(0)\n",
        "\n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        with torch.no_grad:\n",
        "              model.eval() # prep model for evaluation\n",
        "              for data, target in valid_loader:\n",
        "                  # Move input and label tensors to the default device\n",
        "                  data, target = data.to(device), target.to(device)\n",
        "\n",
        "                  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "                  output = model(data)\n",
        "                  # calculate the loss\n",
        "                  loss = criterion(output, target)\n",
        "                  # update running validation loss \n",
        "                  valid_loss += loss.item()*data.size(0)\n",
        "      \n",
        "        # print training/validation statistics \n",
        "        # calculate average loss over an epoch\n",
        "        train_loss = train_loss/len(trainloader.sampler)\n",
        "        valid_loss = valid_loss/len(validloader.sampler)\n",
        "\n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch+1, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "\n",
        "        # save model if validation loss has decreased\n",
        "        if valid_loss <= valid_loss_min:\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            torch.save(model.state_dict(), 'model.pt')\n",
        "            valid_loss_min = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uG1vtBdhSjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(num_epochs):\n",
        "    best_acc = 0.0\n",
        "    \n",
        "    # create a 1x1 grid to display the loss and progress\n",
        "    #grid = widgets.Grid(2,1)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        print('Epoch', epoch)\n",
        "        \n",
        "        # train phase\n",
        "        model.train()\n",
        "        \n",
        "        # create a progress bar\n",
        "        #progress = ProgressMonitor(length=len(train_set))\n",
        "        #train_losses_ma = []\n",
        "        #train_loss_ma = MovingAverage()\n",
        "        \n",
        "        train_acc = 0.0\n",
        "        train_loss = 0.0\n",
        "        # loop over the loader for the training set\n",
        "        for i, (batchImages, labels) in enumerate(train_loader):\n",
        "            # Move images and labels to gpu if available\n",
        "            # if GPU support is available,  move both the images and labels to the GPU\n",
        "            if cuda_avail:\n",
        "                images = Variable(images.cuda())\n",
        "                labels = Variable(labels.cuda())\n",
        "\n",
        "            # Clear all accumulated gradients\n",
        "            # This is important because weights in a neural network are adjusted based on gradients accumulated for each batch, \n",
        "            # hence for each new batch, gradients must be reset to zero, \n",
        "            # so images in a previous batch would not propagate gradients to a new batch.\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Pass our images into the model. \n",
        "            # It returns the predictions, \n",
        "            # and then we pass both the predictions and actual labels into the loss function.\n",
        "            \n",
        "            # forward propagation\n",
        "            # Predict classes\n",
        "            outputs_predictions = model(batchImages)\n",
        "            \n",
        "            # calculate the loss          \n",
        "            # Compute the loss based on the predictions and actual labels\n",
        "            loss = loss_fn(outputs_predictions, labels)\n",
        "            \n",
        "            # call loss.backward() to propagate the gradients, \n",
        "            # and call optimizer.step() to modify our model parameters in accordance with the propagated gradients\n",
        "           \n",
        "            # Backpropagate the loss\n",
        "            loss.backward()\n",
        "              \n",
        "            # update model weights\n",
        "            # Adjust parameters according to the computed gradients\n",
        "            optimizer.step()\n",
        "            \n",
        "            #compute the metrics:\n",
        "            \n",
        "            # update average loss\n",
        "            #train_loss_ma.update(loss)\n",
        "            \n",
        "            # Retrieve the actual loss and then obtain the maximum predicted class.            \n",
        "            train_loss += loss.cpu().data[0] * batchImages.size(0)\n",
        "            _, prediction = torch.max(outputs.data, 1)\n",
        "            # Sum up the number of correct predictions in the batch and add it to the total train_acc\n",
        "            train_acc += torch.sum(prediction == labels.data)\n",
        "            \n",
        "            # update progress bar\n",
        "            #with grid.output_to(0,0):\n",
        "            #    progress.update(batch.shape[0], train_loss)\n",
        "        \n",
        "        \n",
        "        #print('MA Training loss:', train_loss_ma)\n",
        "        #train_losses_ma.append(train_loss_ma.value)\n",
        "        \n",
        "        # Call the learning rate adjustment function\n",
        "        # After each epoch, call the learning rate adjustment function\n",
        "        adjust_learning_rate(epoch)\n",
        "        \n",
        "         \n",
        "        # Compute the training loss and training accuracy, over all # training images\n",
        "        train_acc = train_acc / trainImageCount\n",
        "        train_loss = train_loss / trainImageCount\n",
        "\n",
        "        # Evaluate on valid set\n",
        "        valid_acc = checkAccuracy(valid_loader, validImagesCount)\n",
        "        \n",
        "        # Save the model if the test acc is greater than our current best\n",
        "        if valid_acc > best_acc:\n",
        "            save_models(epoch)\n",
        "            best_acc = valid_acc\n",
        "        \n",
        "            # Save a checkpoint\n",
        "            checkpoint_filename = 'checkpoints/model-{:03d}.pkl'.format(epoch)\n",
        "            save_checkpoint(optimizer, model, epoch, checkpoint_filename)\n",
        "        \n",
        "        \n",
        "        # Plot loss\n",
        "        #with grid.output_to(1, 0):\n",
        "        #    grid.clear_cell()\n",
        "        #    plt.figure(figsize=(10,6))\n",
        "        #    epochs = range(first_epoch, epoch + 1)\n",
        "        #    plt.plot(epochs, train_losses, '-o', label='Training loss')\n",
        "        #    plt.plot(epochs, valid_losses, '-o', label='Validation loss')\n",
        "        #    plt.legend()\n",
        "        #    plt.title('Learning curves')\n",
        "        #    plt.xlabel('Epoch')\n",
        "        #    plt.ylabel('Loss')\n",
        "        #    plt.xticks(epochs)\n",
        "        #    plt.show()\n",
        "        \n",
        "        # Evaluate on the test set\n",
        "        #test_acc = checkAccuracy(test_loader, trainImageCount)\n",
        "\n",
        "     \n",
        "        #if test_acc > best_acc:\n",
        "        #    save_models(epoch)\n",
        "        #    best_acc = test_acc\n",
        "\n",
        "        # Print the metrics\n",
        "        print(\"Epoch {}, Train Accuracy: {} , TrainLoss: {} , Test Accuracy: {}\".format(epoch, train_acc, train_loss,valid_acc))\n",
        "        \n",
        "        \n",
        "                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9DKpuTpqK0J",
        "colab_type": "text"
      },
      "source": [
        "#Start Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU3k4YaInu9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(num_epochs)\n",
        "#if __name__ == \"__main__\":\n",
        "#    train(num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "098kktNduQy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imagenet models are trained with image size 224\n",
        "# cifar10, are trained with image size 32\n",
        "#imageSize = 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD83x_5ksZKh",
        "colab_type": "text"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phlfl5PVsboq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a8825d0c-e8cb-439c-cb65-434f368aff73"
      },
      "source": [
        "# Import needed packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torchvision.models import squeezenet1_1\n",
        "import torch.functional as F\n",
        "import requests\n",
        "import shutil\n",
        "from io import open\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "\"\"\" Instantiate model, this downloads tje 4.7 mb  squzzene the first time it is called.\n",
        "To use with your own model, re-define your trained networks ad load weights as below\n",
        "checkpoint = torch.load(\"pathtosavemodel\")\n",
        "model = SimpleNet(num_classes=10)\n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval()\n",
        "$$$  Note that if your model was trained on ImageNet, then your num_classes must be 1000 instead of 10.\n",
        "$$$  Note if we’re running prediction with a model trained on cifar10, then in the transforms, change transforms.CenterCrop(224) to transforms.Resize(32)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "model = squeezenet1_1(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def predict_image(image_path):\n",
        "    print(\"Prediction in progress\")\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    # Define transformations for the image, should (note that imagenet models are trained with image size 224)\n",
        "    \n",
        "    transformation = transforms.Compose([\n",
        "        transforms.CenterCrop(imageSize),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Preprocess the image\n",
        "    image_tensor = transformation(image).float()\n",
        "\n",
        "    # Add an extra batch dimension since pytorch treats all images as batches\n",
        "    image_tensor = image_tensor.unsqueeze_(0)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        image_tensor.cuda()\n",
        "\n",
        "    # Turn the input into a Variable\n",
        "    input = Variable(image_tensor)\n",
        "\n",
        "    # Predict the class of the image\n",
        "    output = model(input)\n",
        "\n",
        "    index = output.data.numpy().argmax()\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    imagefile = \"image.png\"\n",
        "\n",
        "    imagepath = os.path.join(os.getcwd(), imagefile)\n",
        "    # Donwload image if it doesn't exist\n",
        "    if not os.path.exists(imagepath):\n",
        "        data = requests.get(\n",
        "            \"https://github.com/OlafenwaMoses/ImageAI/raw/master/images/3.jpg\", stream=True)\n",
        "\n",
        "        with open(imagepath, \"wb\") as file:\n",
        "            shutil.copyfileobj(data.raw, file)\n",
        "\n",
        "        del data\n",
        "\n",
        "    index_file = \"class_index_map.json\"\n",
        "\n",
        "    indexpath = os.path.join(os.getcwd(), index_file)\n",
        "    # Donwload class index if it doesn't exist\n",
        "    if not os.path.exists(indexpath):\n",
        "        data = requests.get('https://github.com/OlafenwaMoses/ImageAI/raw/master/imagenet_class_index.json')\n",
        "\n",
        "        with open(indexpath, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(data.text)\n",
        "\n",
        "    class_map = json.load(open(indexpath))\n",
        "\n",
        "    # run prediction function annd obtain prediccted class index\n",
        "    index = predict_image(imagepath)\n",
        "\n",
        "    prediction = class_map[str(index)][1]\n",
        "\n",
        "    print(\"Predicted Class \", prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth\" to /root/.cache/torch/checkpoints/squeezenet1_1-f364aa15.pth\n",
            " 36%|███▌      | 1785856/4966400 [00:00<00:00, 11646119.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68ecea37c0a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqueezenet1_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36msqueezenet1_1\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_squeezenet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36m_squeezenet\u001b[0;34m(version, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'squeezenet'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0;32m--> 110\u001b[0;31m                                               progress=progress)\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0m_download_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/hub.py\u001b[0m in \u001b[0;36m_download_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtXrCMHls9hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}